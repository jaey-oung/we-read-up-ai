from datetime import datetime

from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
from torch.utils.data import Dataset, DataLoader
from model.mbtimlp import MBTIModel
import torch
import torch.nn as nn
import json
import numpy as np
import os
from tqdm import tqdm

# ===== AI ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤ CPUë¡œ ì‹¤í–‰ì‹œ 10ì‹œê°„ ì†Œìš”ë˜ë‹ˆ ì£¼ì˜!!!!!!!!! ===== #

# ===== í˜„ì¬ ì‹œê°„ ì¶œë ¥ í•¨ìˆ˜ ===== #
def currentTime():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


class MBTIDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoded = self.tokenizer(self.texts[idx], padding='max_length',
                                 truncation=True, max_length=256, return_tensors='pt')
        return {
            'input_ids': encoded['input_ids'].squeeze(),
            'attention_mask': encoded['attention_mask'].squeeze(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.float32)
        }

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
print("í›ˆë ¨ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘")
with open('../../data/processed/book_mbti_sample_data.json', 'r', encoding='utf-8') as f:
    data = json.load(f)
texts = [entry['text'] for entry in data]
labels = [list(entry['label'].values()) for entry in data]
labels = np.array(labels, dtype=np.float32) / 100.0
print("í›ˆë ¨ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¢…ë£Œ")

print("í† í¬ë‚˜ì´ì € ì„¤ì •ì¤‘")
# tokenizer ì„¤ì •
tokenizer = AutoTokenizer.from_pretrained("jhgan/ko-sroberta-multitask")
print("í† í¬ë‚˜ì´ì € ì„¤ì • ì¢…ë£Œ")
print("í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘")
# Dataset & DataLoader
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.3)
train_ds = MBTIDataset(train_texts, train_labels, tokenizer)
val_ds = MBTIDataset(val_texts, val_labels, tokenizer)

train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=8)
print("í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¢…ë£Œ")

# ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤í•¨ìˆ˜
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MBTIModel().to(device)
# í•™ìŠµë¥ 
optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)
criterion = nn.MSELoss()

print("Train dataset ê°œìˆ˜:", len(train_loader))
print("Train dataset ê¸¸ì´:", len(train_ds))

# í•™ìŠµ ë£¨í”„
print("í›ˆë ¨ì‹œì‘")
for epoch in range(30):
    model.train()
    total_loss = 0
    print(f"\nğŸ“¢ [Epoch {epoch+1}] ì‹œì‘")

    # tqdmìœ¼ë¡œ í•™ìŠµ ìƒíƒœ í‘œì‹œ
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch}", leave=False)

    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

        # í˜„ì¬ ë°°ì¹˜ ì†ì‹¤ì„ í‘œì‹œ
        progress_bar.set_postfix(loss=loss.item())

    print(f"âœ… [Epoch {epoch+1}] í‰ê·  Loss: {total_loss:.4f}")

print("ìºì‹œ ì €ì¥ íŒŒì¼ ìƒì„±ì¤‘")
    # ì €ì¥ í´ë” ìƒì„±
dest = os.path.join('../../data/cache')
if not os.path.exists(dest):
    os.makedirs(dest)

# í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), os.path.join(dest, 'mbti_model.pt'))
print("ìºì‹œ ì €ì¥ ì™„ë£Œ")
