import json
from collections import Counter

# íŒŒì¼ ê²½ë¡œ
DATA_PATH = '../data/processed/book_mbti_sample_data.json'

# ê¸°ì¤€ê°’
EXTREME_THRESHOLD = 90

# JSON ë¡œë”©
with open(DATA_PATH, 'r', encoding='utf-8') as f:
    data = json.load(f)

# ê·¹ë‹¨ê°’ê³¼ ì¼ë°˜ê°’ ë¶„ë¦¬
extreme_samples = []
normal_samples = []
extreme_feature_count = Counter()

# ìˆ˜ì •ëœ í•­ëª© ê¸°ë¡ìš©
normalization_logs = []

def normalize_pair(d, key1, key2):
    v1, v2 = d.get(key1, 0), d.get(key2, 0)
    total = v1 + v2
    if total == 0:
        return 50, 50  # ë‘˜ ë‹¤ 0ì´ë©´ ê· ë“± ë¶„í• 
    elif total == 100:
        return v1, v2
    else:
        ratio1 = v1 / total
        ratio2 = v2 / total
        new_v1 = round(ratio1 * 100)
        new_v2 = 100 - new_v1  # ì†Œìˆ˜ ë°˜ì˜¬ë¦¼ ë³´ì •
        return new_v1, new_v2

# ì „ì²´ ë°ì´í„° ìˆœíšŒ
for idx, item in enumerate(data):
    score_dict = item.get("scores", item.get("label", {}))

    # ì›ë³¸ ê°’ ì €ì¥
    original = score_dict.copy()

    # ì •ê·œí™” ìˆ˜í–‰
    s, i = normalize_pair(score_dict, "S", "I")
    f, d = normalize_pair(score_dict, "F", "D")
    n, m = normalize_pair(score_dict, "N", "M")
    q, a = normalize_pair(score_dict, "Q", "A")

    # ì—…ë°ì´íŠ¸
    score_dict.update({"S": s, "I": i, "F": f, "D": d, "N": n, "M": m, "Q": q, "A": a})

    # ìˆ˜ì •ì‚¬í•­ ê¸°ë¡
    if original != score_dict:
        normalization_logs.append({
            "index": idx,
            "original": original,
            "normalized": score_dict.copy()
        })

    # ê·¹ë‹¨ê°’ ë¶„ë¥˜
    extreme_keys = [k for k, v in score_dict.items() if v >= EXTREME_THRESHOLD]
    if extreme_keys:
        extreme_samples.append(item)
        extreme_feature_count.update(extreme_keys)
    else:
        normal_samples.append(item)

# ì •ë¦¬ëœ ìˆœì„œ: ì¼ë°˜ê°’ â†’ ê·¹ë‹¨ê°’
output_data = normal_samples + extreme_samples

# ë®ì–´ì“°ê¸° ì €ì¥
with open(DATA_PATH, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)

# ê²°ê³¼ ì¶œë ¥
print(f"âœ… ì •ë¦¬ ì™„ë£Œ: ì¼ë°˜ê°’ {len(normal_samples)}ê±´ / ê·¹ë‹¨ê°’ {len(extreme_samples)}ê±´")
print(f"ğŸ“Š ê·¹ë‹¨ê°’ ê¸°ì¤€ í•­ëª© ë¶„í¬ (ê¸°ì¤€: {EXTREME_THRESHOLD} ì´ìƒ):")
for k, v in extreme_feature_count.most_common():
    print(f"  - {k}: {v}ê±´")

# ì •ê·œí™” ë¡œê·¸ ì¶œë ¥
if normalization_logs:
    print(f"\nğŸ”§ ì´ {len(normalization_logs)}ê±´ì˜ ë°ì´í„°ì—ì„œ í•©ì´ 100ì´ ë˜ë„ë¡ ì •ê·œí™” ë˜ì—ˆìŠµë‹ˆë‹¤:")
else:
    print("\nâœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ S+I, F+D, N+M, Q+A í•© 100ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")
